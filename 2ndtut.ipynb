{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basics of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.5,0.1])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2417, 0.6173],\n",
      "        [0.4150, 0.6837]])\n",
      "tensor([[0.1965, 0.5960],\n",
      "        [0.4553, 0.2973]])\n",
      "tensor([[0.4382, 1.2133],\n",
      "        [0.8703, 0.9810]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "# y.add_(x)\n",
    "# print(y)\n",
    "print(x)\n",
    "print(y)\n",
    "# z= x+y\n",
    "z = torch.add(x,y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8213, 0.6431, 0.7928, 0.3410],\n",
      "        [0.1532, 0.2795, 0.6350, 0.9342],\n",
      "        [0.1054, 0.9221, 0.9123, 0.3146],\n",
      "        [0.9175, 0.1928, 0.5509, 0.6039]])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "y= x.view(-1,8)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a= torch.ones(5)\n",
    "# print(a)\n",
    "b = a.numpy()\n",
    "# print(b)\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(5,device=device)\n",
    "    y = torch.ones(5)\n",
    "    y = y.to(device)\n",
    "    z = x + y\n",
    "    ## numpy only works on cpu\n",
    "    # z.numpy()\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7360, 0.9878, 0.2846], requires_grad=True)\n",
      "tensor([2.7360, 2.9878, 2.2846], grad_fn=<AddBackward0>)\n",
      "tensor(14.4214, grad_fn=<MeanBackward0>)\n",
      "tensor([3.6480, 3.9838, 3.0461])\n"
     ]
    }
   ],
   "source": [
    "## requires grad = true makes the jacobian automatically\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y*y*2\n",
    "z = z.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward() #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7527, 0.8423, 0.6666])\n",
      "tensor([2.7527, 2.8423, 2.6666])\n",
      "tensor(15.1779)\n"
     ]
    }
   ],
   "source": [
    "## requires grad = false, so back propagation wont work \n",
    "x = torch.randn(3, requires_grad=False)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y*y*2\n",
    "z = z.mean()\n",
    "print(z)\n",
    "\n",
    "# z.backward() #dz/dx\n",
    "# print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.5941,  0.5124, -0.2263], requires_grad=True)\n",
      "tensor([2.5941, 2.5124, 1.7737], grad_fn=<AddBackward0>)\n",
      "tensor([13.4584, 12.6248,  6.2919], grad_fn=<MulBackward0>)\n",
      "tensor([1.0376e+00, 1.0050e+01, 7.0947e-03])\n"
     ]
    }
   ],
   "source": [
    "## to make backprop work we need to enter a jacobian manually then\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y*y*2\n",
    "# z = z.mean()\n",
    "print(z)\n",
    "\n",
    "v = torch.tensor([0.1,1.0,0.001] , dtype=torch.float32) # jacobian vector\n",
    "z.backward(v) #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 ways to make sure torch does not track history of grad function in the computation graph as when we update weights during backprop we dont want it to track\n",
    "# x.requires_grad_(False)\n",
    "# x.detach()\n",
    "# with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6097,  0.6497, -2.9280], requires_grad=True)\n",
      "tensor([-1.6097,  0.6497, -2.9280])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4678,  1.5882, -0.9091], requires_grad=True)\n",
      "tensor([ 0.4678,  1.5882, -0.9091])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8571, 0.2766, 0.4322], requires_grad=True)\n",
      "tensor([2.8571, 2.2766, 2.4322])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epocch in range(3):\n",
    "    model_ouput = (weights*3).sum()\n",
    "\n",
    "    model_ouput.backward()\n",
    "\n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_() # empty the grad or it will sum up everything "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forwaed pass and compute the loss\n",
    "y_hat = w*x\n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "###update weights\n",
    "###next forward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction : Manual,\n",
    "Gradients Computation : Manual,\n",
    "Loss computation : Manual,\n",
    "Parameter updates : Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1 : w = 1.200, loss = 30.00000000\n",
      "epoch 3 : w = 1.872, loss = 0.76800019\n",
      "epoch 5 : w = 1.980, loss = 0.01966083\n",
      "epoch 7 : w = 1.997, loss = 0.00050331\n",
      "epoch 9 : w = 1.999, loss = 0.00001288\n",
      "epoch 11 : w = 2.000, loss = 0.00000033\n",
      "epoch 13 : w = 2.000, loss = 0.00000001\n",
      "epoch 15 : w = 2.000, loss = 0.00000000\n",
      "epoch 17 : w = 2.000, loss = 0.00000000\n",
      "epoch 19 : w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "## f = w*x \n",
    "# f = 2*x\n",
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype= np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x) :\n",
    "    return w * x\n",
    "\n",
    "# loss\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x * (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "## Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    #gradients\n",
    "    dw = gradient(X,Y,y_pred)\n",
    "\n",
    "    #update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if(epoch%2) == 0 :\n",
    "        print(f'epoch {epoch + 1} : w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction : Manual,\n",
    "Gradients Computation : Autograd,\n",
    "Loss computation : Manual,\n",
    "Parameter updates : Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1 : w = 0.300, loss = 30.00000000\n",
      "epoch 11 : w = 1.665, loss = 1.16278565\n",
      "epoch 21 : w = 1.934, loss = 0.04506890\n",
      "epoch 31 : w = 1.987, loss = 0.00174685\n",
      "epoch 41 : w = 1.997, loss = 0.00006770\n",
      "epoch 51 : w = 1.999, loss = 0.00000262\n",
      "epoch 61 : w = 2.000, loss = 0.00000010\n",
      "epoch 71 : w = 2.000, loss = 0.00000000\n",
      "epoch 81 : w = 2.000, loss = 0.00000000\n",
      "epoch 91 : w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "## f = w*x \n",
    "# f = 2*x\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype= torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x) :\n",
    "    return w * x\n",
    "\n",
    "# loss\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "## Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "\n",
    "    #update weights\n",
    "    with torch.no_grad(): \n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if(epoch%10) == 0 :\n",
    "        print(f'epoch {epoch + 1} : w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction : Manual,\n",
    "Gradients Computation : Autograd, \n",
    "Loss computation : Pytorch Loss, \n",
    "Parameter updates : Pytorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1 : w = 0.300, loss = 30.00000000\n",
      "epoch 11 : w = 1.665, loss = 1.16278565\n",
      "epoch 21 : w = 1.934, loss = 0.04506890\n",
      "epoch 31 : w = 1.987, loss = 0.00174685\n",
      "epoch 41 : w = 1.997, loss = 0.00006770\n",
      "epoch 51 : w = 1.999, loss = 0.00000262\n",
      "epoch 61 : w = 2.000, loss = 0.00000010\n",
      "epoch 71 : w = 2.000, loss = 0.00000000\n",
      "epoch 81 : w = 2.000, loss = 0.00000000\n",
      "epoch 91 : w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "## f = w*x \n",
    "# f = 2*x\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype= torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x) :\n",
    "    return w * x\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "## Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w],lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if(epoch%10) == 0 :\n",
    "        print(f'epoch {epoch + 1} : w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction : Pytorch Model,\n",
    "Gradients Computation : Autograd, \n",
    "Loss computation : Pytorch Loss, \n",
    "Parameter updates : Pytorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 4.818\n",
      "epoch 1 : w = 1.016, loss = 7.14700794\n",
      "epoch 11 : w = 1.631, loss = 0.27471912\n",
      "epoch 21 : w = 1.737, loss = 0.09168920\n",
      "epoch 31 : w = 1.760, loss = 0.08203067\n",
      "epoch 41 : w = 1.769, loss = 0.07714427\n",
      "epoch 51 : w = 1.776, loss = 0.07265113\n",
      "epoch 61 : w = 1.783, loss = 0.06842241\n",
      "epoch 71 : w = 1.789, loss = 0.06443992\n",
      "epoch 81 : w = 1.796, loss = 0.06068916\n",
      "epoch 91 : w = 1.802, loss = 0.05715669\n",
      "Prediction after training: f(5) = 9.602\n"
     ]
    }
   ],
   "source": [
    "## f = w*x \n",
    "# f = 2*x\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype= torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5] , dtype=torch.float32)\n",
    "n_samples , n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module) :\n",
    "    def __init__(self,input_dim , output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size,output_size)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "## Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if(epoch%10) == 0 :\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1} : w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression\n",
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2 ) Construct loss and optimizer\n",
    "# 3 ) Training Loop \n",
    "#  - forward pass : compute prediction \n",
    "#  - backward pass : gradients\n",
    "#  - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 , loss = 4409.7749\n",
      "epoch: 20 , loss = 3289.0596\n",
      "epoch: 30 , loss = 2478.3396\n",
      "epoch: 40 , loss = 1891.2338\n",
      "epoch: 50 , loss = 1465.6367\n",
      "epoch: 60 , loss = 1156.8328\n",
      "epoch: 70 , loss = 932.5792\n",
      "epoch: 80 , loss = 769.5974\n",
      "epoch: 90 , loss = 651.0603\n",
      "epoch: 100 , loss = 564.7903\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples = 100, n_features =1, noise = 20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0],1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# model\n",
    "\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# loss and optim\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, y)\n",
    "\n",
    "    # backwawrd pass\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if(epoch + 1) % 10 == 0 :\n",
    "        print(f'epoch: {epoch+1} , loss = {loss.item():.4f}')\n",
    "\n",
    "#plot\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2 ) Construct loss and optimizer\n",
    "# 3 ) Training Loop \n",
    "#  - forward pass : compute prediction \n",
    "#  - backward pass : gradients\n",
    "#  - update weights\n",
    "\n",
    "add one more layer and different loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10, loss = 0.4744\n",
      "epoch : 20, loss = 0.4087\n",
      "epoch : 30, loss = 0.3632\n",
      "epoch : 40, loss = 0.3298\n",
      "epoch : 50, loss = 0.3040\n",
      "epoch : 60, loss = 0.2835\n",
      "epoch : 70, loss = 0.2667\n",
      "epoch : 80, loss = 0.2526\n",
      "epoch : 90, loss = 0.2406\n",
      "epoch : 100, loss = 0.2302\n",
      "accuracy = 0.9123\n"
     ]
    }
   ],
   "source": [
    "## 0) prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X,y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=1234)\n",
    "\n",
    "#scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)\n",
    "\n",
    "## 1) model\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module) :\n",
    "    def __init__(self, n_input_features) -> None:\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "## 2) loss and opitm\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "## 3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #updates\n",
    "    optimizer.step()\n",
    "\n",
    "    #zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if(epoch + 1)%10 == 0 :\n",
    "        print(f'epoch : {epoch + 1}, loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and dataloader\n",
    "\n",
    "epcoh = 1 forward and backward pass of ALL training examples\n",
    "\n",
    "batch_size = no.of training samples in one forward & backward pass\n",
    "\n",
    "no.of iters = number of passes, each pass using [batch_size] nno.of samples\n",
    "\n",
    "e.g. 100 samples, batch_size = 20 => 100 / 20 = 5 iterations for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n",
      "epoch 1/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 45/45, inputs torch.Size([2, 13])\n",
      "epoch 2/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 45/45, inputs torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__ (self):\n",
    "        # data loading\n",
    "        xy = np.loadtxt('wine.csv', delimiter=\",\", dtype=np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:, 1:])\n",
    "        self.y = torch.from_numpy(xy[:, [0]]) # n_samples , 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        # dataset[0]\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "    \n",
    "dataset = WineDataset()\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers = 0)\n",
    "\n",
    "# dataiter = iter(dataloader)\n",
    "# data = next(dataiter)\n",
    "# features, labels = data\n",
    "# print(features)\n",
    "# print(labels)\n",
    "\n",
    "# traning loop\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs) :\n",
    "    for i, (inputs, labels) in enumerate(dataloader) :\n",
    "        # forward backward, update\n",
    "        if( i+1) % 5 == 0 :\n",
    "            print( f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms and MNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "tensor([  16.2300,    3.7100,    4.4300,   17.6000,  129.0000,    4.8000,\n",
      "           5.0600,    2.2800,    4.2900,    7.6400,    3.0400,    5.9200,\n",
      "        1067.0000])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__ (self, transfrom = None):\n",
    "        # data loading\n",
    "        xy = np.loadtxt('wine.csv', delimiter=\",\", dtype=np.float32, skiprows=1)\n",
    "        \n",
    "        # note that we do not convert to tensor here\n",
    "        self.x = xy[:, 1:]\n",
    "        self.y = xy[:, [0]] # n_samples , 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "        self.transfrom = transfrom\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        # dataset[0]\n",
    "        sample = self.x[index], self.y[index]\n",
    "        if self.transfrom :\n",
    "            sample = self.transfrom(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "\n",
    "class toTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "    \n",
    "class mulTransfrom:\n",
    "    def __init__ (self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__ (self, sample):\n",
    "        inputs, target = sample\n",
    "        inputs += self.factor\n",
    "        return inputs, target\n",
    "    \n",
    "dataset = WineDataset(transfrom=None)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels))\n",
    "\n",
    "composed = torchvision.transforms.Compose([toTensor(),mulTransfrom(2)])\n",
    "dataset = WineDataset(transfrom=composed)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax and cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
      "tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print('softmax numpy:', outputs)\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 numpy: 0.3567\n",
      "Loss2 numpy: 2.3026\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(actual , predicted) :\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss # / float(predicted.shape[0])\n",
    "# y must be one hot encoded\n",
    "# if class 0 : [1 0 0]\n",
    "# if class 1 : [0 1 0]\n",
    "# if class 2 : [0 0 1]\n",
    "Y = np.array([1,0,0])\n",
    "\n",
    "# y_pred has probabilities\n",
    "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "l1 = cross_entropy(Y, Y_pred_good)\n",
    "l2 = cross_entropy(Y, Y_pred_bad)\n",
    "print(f'Loss1 numpy: {l1:.4f}')\n",
    "print(f'Loss2 numpy: {l2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.CrossEntropyLoss applies\n",
    "## nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
    "--> No Softmax in last layer\n",
    "Y has class labels, not One - Hot!\n",
    "Y_pred has raw scores(logits), no Softmax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3018244206905365\n",
      "1.6241613626480103\n",
      "tensor([2, 0, 1])\n",
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3 samples\n",
    "Y = torch.tensor([2,0,1])\n",
    "# n_samples x nclasses = 3x3\n",
    "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1],[2.0, 1.0, 0.1],[0.1, 3.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1],[0.1, 1.0, 2.1],[0.1, 3.0, 0.1]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1.item())\n",
    "print(l2.item())\n",
    "\n",
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "print(predictions1)\n",
    "print(predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # no softmax at end\n",
    "        return out\n",
    "    \n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss() #applies softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in case of binary classification ( is it a dog or not)\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "    \n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "1. Step function\n",
    "2. Sigmoid\n",
    "3. TanH\n",
    "4. ReLU\n",
    "5. Leaky ReLU\n",
    "6. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1 (create nn modeules)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2 (use activation functiosn directly in forward pass)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__ (self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
