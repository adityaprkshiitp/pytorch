{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basics of pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.5,0.1])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5777, 0.9790],\n",
      "        [0.8853, 0.3145]])\n",
      "tensor([[0.3340, 0.6503],\n",
      "        [0.3245, 0.4697]])\n",
      "tensor([[0.9117, 1.6292],\n",
      "        [1.2098, 0.7842]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "# y.add_(x)\n",
    "# print(y)\n",
    "print(x)\n",
    "print(y)\n",
    "# z= x+y\n",
    "z = torch.add(x,y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7822, 0.9312, 0.4791, 0.1042],\n",
      "        [0.0084, 0.3592, 0.2087, 0.3380],\n",
      "        [0.9109, 0.5431, 0.5666, 0.2055],\n",
      "        [0.1395, 0.0359, 0.4602, 0.3941]])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(4,4)\n",
    "print(x)\n",
    "y= x.view(-1,8)\n",
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a= torch.ones(5)\n",
    "# print(a)\n",
    "b = a.numpy()\n",
    "# print(b)\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(5,device=device)\n",
    "    y = torch.ones(5)\n",
    "    y = y.to(device)\n",
    "    z = x + y\n",
    "    ## numpy only works on cpu\n",
    "    # z.numpy()\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2425, -0.4618,  2.0889], requires_grad=True)\n",
      "tensor([1.7575, 1.5382, 4.0889], grad_fn=<AddBackward0>)\n",
      "tensor(14.7825, grad_fn=<MeanBackward0>)\n",
      "tensor([2.3434, 2.0509, 5.4518])\n"
     ]
    }
   ],
   "source": [
    "## requires grad = true makes the jacobian automatically\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y*y*2\n",
    "z = z.mean()\n",
    "print(z)\n",
    "\n",
    "z.backward() #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4571, 1.0218, 0.5140])\n",
      "tensor([2.4571, 3.0218, 2.5140])\n",
      "tensor(14.3259)\n"
     ]
    }
   ],
   "source": [
    "## requires grad = false, so back propagation wont work \n",
    "x = torch.randn(3, requires_grad=False)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y*y*2\n",
    "z = z.mean()\n",
    "print(z)\n",
    "\n",
    "# z.backward() #dz/dx\n",
    "# print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.3607, -0.0315, -0.9800], requires_grad=True)\n",
      "tensor([2.3607, 1.9685, 1.0200], grad_fn=<AddBackward0>)\n",
      "tensor([11.1458,  7.7499,  2.0808], grad_fn=<MulBackward0>)\n",
      "tensor([9.4428e-01, 7.8739e+00, 4.0800e-03])\n"
     ]
    }
   ],
   "source": [
    "## to make backprop work we need to enter a jacobian manually then\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y*y*2\n",
    "# z = z.mean()\n",
    "print(z)\n",
    "\n",
    "v = torch.tensor([0.1,1.0,0.001] , dtype=torch.float32) # jacobian vector\n",
    "z.backward(v) #dz/dx\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 ways to make sure torch does not track history of grad function in the computation graph as when we update weights during backprop we dont want it to track\n",
    "# x.requires_grad_(False)\n",
    "# x.detach()\n",
    "# with torch.no_grad():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3562, 1.4153, 0.1986], requires_grad=True)\n",
      "tensor([0.3562, 1.4153, 0.1986])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.6581, -0.5160, -0.5003], requires_grad=True)\n",
      "tensor([-1.6581, -0.5160, -0.5003])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "y = x.detach()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.1663,  0.0958, -0.8502], requires_grad=True)\n",
      "tensor([2.1663, 2.0958, 1.1498])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "with torch.no_grad():\n",
    "    y = x + 2\n",
    "    print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n",
      "tensor([3., 3., 3., 3.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "for epocch in range(3):\n",
    "    model_ouput = (weights*3).sum()\n",
    "\n",
    "    model_ouput.backward()\n",
    "\n",
    "    print(weights.grad)\n",
    "\n",
    "    weights.grad.zero_() # empty the grad or it will sum up everything "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "# forwaed pass and compute the loss\n",
    "y_hat = w*x\n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "print(loss)\n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "###update weights\n",
    "###next forward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction : Manual,\n",
    "Gradients Computation : Manual,\n",
    "Loss computation : Manual,\n",
    "Parameter updates : Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1 : w = 1.200, loss = 30.00000000\n",
      "epoch 3 : w = 1.872, loss = 0.76800019\n",
      "epoch 5 : w = 1.980, loss = 0.01966083\n",
      "epoch 7 : w = 1.997, loss = 0.00050331\n",
      "epoch 9 : w = 1.999, loss = 0.00001288\n",
      "epoch 11 : w = 2.000, loss = 0.00000033\n",
      "epoch 13 : w = 2.000, loss = 0.00000001\n",
      "epoch 15 : w = 2.000, loss = 0.00000000\n",
      "epoch 17 : w = 2.000, loss = 0.00000000\n",
      "epoch 19 : w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "## f = w*x \n",
    "# f = 2*x\n",
    "X = np.array([1,2,3,4], dtype=np.float32)\n",
    "Y = np.array([2,4,6,8], dtype= np.float32)\n",
    "\n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x) :\n",
    "    return w * x\n",
    "\n",
    "# loss\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N * 2x * (w*x - y)\n",
    "def gradient(x,y,y_predicted):\n",
    "    return np.dot(2*x, y_predicted-y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "## Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    #gradients\n",
    "    dw = gradient(X,Y,y_pred)\n",
    "\n",
    "    #update weights\n",
    "    w -= learning_rate * dw\n",
    "\n",
    "    if(epoch%2) == 0 :\n",
    "        print(f'epoch {epoch + 1} : w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction : Manual,\n",
    "Gradients Computation : Autograd,\n",
    "Loss computation : Manual,\n",
    "Parameter updates : Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1 : w = 0.300, loss = 30.00000000\n",
      "epoch 11 : w = 1.665, loss = 1.16278565\n",
      "epoch 21 : w = 1.934, loss = 0.04506890\n",
      "epoch 31 : w = 1.987, loss = 0.00174685\n",
      "epoch 41 : w = 1.997, loss = 0.00006770\n",
      "epoch 51 : w = 1.999, loss = 0.00000262\n",
      "epoch 61 : w = 2.000, loss = 0.00000010\n",
      "epoch 71 : w = 2.000, loss = 0.00000000\n",
      "epoch 81 : w = 2.000, loss = 0.00000000\n",
      "epoch 91 : w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "## f = w*x \n",
    "# f = 2*x\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype= torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x) :\n",
    "    return w * x\n",
    "\n",
    "# loss\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted-y)**2).mean()\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "## Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "\n",
    "    #update weights\n",
    "    with torch.no_grad(): \n",
    "        w -= learning_rate * w.grad\n",
    "\n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "\n",
    "    if(epoch%10) == 0 :\n",
    "        print(f'epoch {epoch + 1} : w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction : Manual,\n",
    "Gradients Computation : Autograd, \n",
    "Loss computation : Pytorch Loss, \n",
    "Parameter updates : Pytorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "epoch 1 : w = 0.300, loss = 30.00000000\n",
      "epoch 11 : w = 1.665, loss = 1.16278565\n",
      "epoch 21 : w = 1.934, loss = 0.04506890\n",
      "epoch 31 : w = 1.987, loss = 0.00174685\n",
      "epoch 41 : w = 1.997, loss = 0.00006770\n",
      "epoch 51 : w = 1.999, loss = 0.00000262\n",
      "epoch 61 : w = 2.000, loss = 0.00000010\n",
      "epoch 71 : w = 2.000, loss = 0.00000000\n",
      "epoch 81 : w = 2.000, loss = 0.00000000\n",
      "epoch 91 : w = 2.000, loss = 0.00000000\n",
      "Prediction after training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "## f = w*x \n",
    "# f = 2*x\n",
    "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype= torch.float32)\n",
    "\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x) :\n",
    "    return w * x\n",
    "\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "## Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w],lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if(epoch%10) == 0 :\n",
    "        print(f'epoch {epoch + 1} : w = {w:.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction : Pytorch Model,\n",
    "Gradients Computation : Autograd, \n",
    "Loss computation : Pytorch Loss, \n",
    "Parameter updates : Pytorch Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 2.071\n",
      "epoch 1 : w = 0.522, loss = 17.09280586\n",
      "epoch 11 : w = 1.474, loss = 0.61065775\n",
      "epoch 21 : w = 1.636, loss = 0.17442229\n",
      "epoch 31 : w = 1.670, loss = 0.15390323\n",
      "epoch 41 : w = 1.684, loss = 0.14467700\n",
      "epoch 51 : w = 1.694, loss = 0.13624908\n",
      "epoch 61 : w = 1.703, loss = 0.12831852\n",
      "epoch 71 : w = 1.712, loss = 0.12084970\n",
      "epoch 81 : w = 1.720, loss = 0.11381563\n",
      "epoch 91 : w = 1.728, loss = 0.10719097\n",
      "Prediction after training: f(5) = 9.455\n"
     ]
    }
   ],
   "source": [
    "## f = w*x \n",
    "# f = 2*x\n",
    "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2],[4],[6],[8]], dtype= torch.float32)\n",
    "\n",
    "X_test = torch.tensor([5] , dtype=torch.float32)\n",
    "n_samples , n_features = X.shape\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "# model = nn.Linear(input_size, output_size)\n",
    "\n",
    "class LinearRegression(nn.Module) :\n",
    "    def __init__(self,input_dim , output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size,output_size)\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
    "\n",
    "## Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    #prediction = forward pass\n",
    "    y_pred = model(X)\n",
    "\n",
    "    #loss\n",
    "    l = loss(Y,y_pred)\n",
    "\n",
    "    #gradients = backward pass\n",
    "    l.backward() # dl/dw\n",
    "\n",
    "    #update weights\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if(epoch%10) == 0 :\n",
    "        [w,b] = model.parameters()\n",
    "        print(f'epoch {epoch + 1} : w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "\n",
    "\n",
    "print(f'Prediction after training: f(5) = {model(X_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression\n",
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2 ) Construct loss and optimizer\n",
    "# 3 ) Training Loop \n",
    "#  - forward pass : compute prediction \n",
    "#  - backward pass : gradients\n",
    "#  - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 , loss = 4312.2910"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 20 , loss = 3217.3940\n",
      "epoch: 30 , loss = 2425.6709\n",
      "epoch: 40 , loss = 1852.5363\n",
      "epoch: 50 , loss = 1437.2126\n",
      "epoch: 60 , loss = 1135.9590\n",
      "epoch: 70 , loss = 917.2531\n",
      "epoch: 80 , loss = 758.3470\n",
      "epoch: 90 , loss = 642.8031\n",
      "epoch: 100 , loss = 558.7312\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare data\n",
    "\n",
    "X_numpy, y_numpy = datasets.make_regression(n_samples = 100, n_features =1, noise = 20, random_state=1)\n",
    "\n",
    "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "y = y.view(y.shape[0],1)\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# model\n",
    "\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# loss and optim\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, y)\n",
    "\n",
    "    # backwawrd pass\n",
    "    loss.backward()\n",
    "\n",
    "    #update\n",
    "    optimizer.step()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if(epoch + 1) % 10 == 0 :\n",
    "        print(f'epoch: {epoch+1} , loss = {loss.item():.4f}')\n",
    "\n",
    "#plot\n",
    "predicted = model(X).detach().numpy()\n",
    "plt.plot(X_numpy, y_numpy, 'ro')\n",
    "plt.plot(X_numpy, predicted, 'b')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression\n",
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2 ) Construct loss and optimizer\n",
    "# 3 ) Training Loop \n",
    "#  - forward pass : compute prediction \n",
    "#  - backward pass : gradients\n",
    "#  - update weights\n",
    "\n",
    "add one more layer and different loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 10, loss = 0.5845\n",
      "epoch : 20, loss = 0.4759\n",
      "epoch : 30, loss = 0.4079\n",
      "epoch : 40, loss = 0.3615\n",
      "epoch : 50, loss = 0.3277\n",
      "epoch : 60, loss = 0.3019\n",
      "epoch : 70, loss = 0.2814\n",
      "epoch : 80, loss = 0.2648\n",
      "epoch : 90, loss = 0.2509\n",
      "epoch : 100, loss = 0.2390\n",
      "accuracy = 0.9123\n"
     ]
    }
   ],
   "source": [
    "## 0) prepare data\n",
    "bc = datasets.load_breast_cancer()\n",
    "X,y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state=1234)\n",
    "\n",
    "#scale\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "y_train = y_train.view(y_train.shape[0],1)\n",
    "y_test = y_test.view(y_test.shape[0],1)\n",
    "\n",
    "## 1) model\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module) :\n",
    "    def __init__(self, n_input_features) -> None:\n",
    "        super(LogisticRegression,self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "## 2) loss and opitm\n",
    "learning_rate = 0.01\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "## 3) training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # forward\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    #updates\n",
    "    optimizer.step()\n",
    "\n",
    "    #zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if(epoch + 1)%10 == 0 :\n",
    "        print(f'epoch : {epoch + 1}, loss = {loss.item():.4f}')\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset and dataloader\n",
    "\n",
    "epcoh = 1 forward and backward pass of ALL training examples\n",
    "\n",
    "batch_size = no.of training samples in one forward & backward pass\n",
    "\n",
    "no.of iters = number of passes, each pass using [batch_size] nno.of samples\n",
    "\n",
    "e.g. 100 samples, batch_size = 20 => 100 / 20 = 5 iterations for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 45\n",
      "epoch 1/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 1/2, step 45/45, inputs torch.Size([2, 13])\n",
      "epoch 2/2, step 5/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 10/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 15/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 20/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 25/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 30/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 35/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 40/45, inputs torch.Size([4, 13])\n",
      "epoch 2/2, step 45/45, inputs torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__ (self):\n",
    "        # data loading\n",
    "        xy = np.loadtxt('wine.csv', delimiter=\",\", dtype=np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:, 1:])\n",
    "        self.y = torch.from_numpy(xy[:, [0]]) # n_samples , 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        # dataset[0]\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "    \n",
    "dataset = WineDataset()\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers = 0)\n",
    "\n",
    "# dataiter = iter(dataloader)\n",
    "# data = next(dataiter)\n",
    "# features, labels = data\n",
    "# print(features)\n",
    "# print(labels)\n",
    "\n",
    "# traning loop\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples/4)\n",
    "print(total_samples, n_iterations)\n",
    "\n",
    "for epoch in range(num_epochs) :\n",
    "    for i, (inputs, labels) in enumerate(dataloader) :\n",
    "        # forward backward, update\n",
    "        if( i+1) % 5 == 0 :\n",
    "            print( f'epoch {epoch+1}/{num_epochs}, step {i+1}/{n_iterations}, inputs {inputs.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms and MNIST datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.423e+01 1.710e+00 2.430e+00 1.560e+01 1.270e+02 2.800e+00 3.060e+00\n",
      " 2.800e-01 2.290e+00 5.640e+00 1.040e+00 3.920e+00 1.065e+03]\n",
      "<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
      "tensor([  16.2300,    3.7100,    4.4300,   17.6000,  129.0000,    4.8000,\n",
      "           5.0600,    2.2800,    4.2900,    7.6400,    3.0400,    5.9200,\n",
      "        1067.0000])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "class WineDataset(Dataset):\n",
    "    def __init__ (self, transfrom = None):\n",
    "        # data loading\n",
    "        xy = np.loadtxt('wine.csv', delimiter=\",\", dtype=np.float32, skiprows=1)\n",
    "        \n",
    "        # note that we do not convert to tensor here\n",
    "        self.x = xy[:, 1:]\n",
    "        self.y = xy[:, [0]] # n_samples , 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "\n",
    "        self.transfrom = transfrom\n",
    "\n",
    "    def __getitem__(self, index) :\n",
    "        # dataset[0]\n",
    "        sample = self.x[index], self.y[index]\n",
    "        if self.transfrom :\n",
    "            sample = self.transfrom(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "\n",
    "class toTensor:\n",
    "    def __call__(self, sample):\n",
    "        inputs, targets = sample\n",
    "        return torch.from_numpy(inputs), torch.from_numpy(targets)\n",
    "    \n",
    "class mulTransfrom:\n",
    "    def __init__ (self, factor):\n",
    "        self.factor = factor\n",
    "\n",
    "    def __call__ (self, sample):\n",
    "        inputs, target = sample\n",
    "        inputs += self.factor\n",
    "        return inputs, target\n",
    "    \n",
    "dataset = WineDataset(transfrom=None)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels))\n",
    "\n",
    "composed = torchvision.transforms.Compose([toTensor(),mulTransfrom(2)])\n",
    "dataset = WineDataset(transfrom=composed)\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features)\n",
    "print(type(features), type(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax and cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax numpy: [0.65900114 0.24243297 0.09856589]\n",
      "tensor([0.6590, 0.2424, 0.0986])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis = 0)\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print('softmax numpy:', outputs)\n",
    "\n",
    "x = torch.tensor([2.0, 1.0, 0.1])\n",
    "outputs = torch.softmax(x, dim=0)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss1 numpy: 0.3567\n",
      "Loss2 numpy: 2.3026\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy(actual , predicted) :\n",
    "    loss = -np.sum(actual * np.log(predicted))\n",
    "    return loss # / float(predicted.shape[0])\n",
    "# y must be one hot encoded\n",
    "# if class 0 : [1 0 0]\n",
    "# if class 1 : [0 1 0]\n",
    "# if class 2 : [0 0 1]\n",
    "Y = np.array([1,0,0])\n",
    "\n",
    "# y_pred has probabilities\n",
    "Y_pred_good = np.array([0.7, 0.2, 0.1])\n",
    "Y_pred_bad = np.array([0.1, 0.3, 0.6])\n",
    "l1 = cross_entropy(Y, Y_pred_good)\n",
    "l2 = cross_entropy(Y, Y_pred_bad)\n",
    "print(f'Loss1 numpy: {l1:.4f}')\n",
    "print(f'Loss2 numpy: {l2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.CrossEntropyLoss applies\n",
    "## nn.LogSoftmax + nn.NLLLoss (negative log likelihood loss)\n",
    "--> No Softmax in last layer\n",
    "Y has class labels, not One - Hot!\n",
    "Y_pred has raw scores(logits), no Softmax!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3018244206905365\n",
      "1.6241613626480103\n",
      "tensor([2, 0, 1])\n",
      "tensor([0, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3 samples\n",
    "Y = torch.tensor([2,0,1])\n",
    "# n_samples x nclasses = 3x3\n",
    "Y_pred_good = torch.tensor([[0.1, 1.0, 2.1],[2.0, 1.0, 0.1],[0.1, 3.0, 0.1]])\n",
    "Y_pred_bad = torch.tensor([[2.1, 1.0, 0.1],[0.1, 1.0, 2.1],[0.1, 3.0, 0.1]])\n",
    "\n",
    "l1 = loss(Y_pred_good, Y)\n",
    "l2 = loss(Y_pred_bad, Y)\n",
    "\n",
    "print(l1.item())\n",
    "print(l2.item())\n",
    "\n",
    "_, predictions1 = torch.max(Y_pred_good, 1)\n",
    "_, predictions2 = torch.max(Y_pred_bad, 1)\n",
    "print(predictions1)\n",
    "print(predictions2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet2, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # no softmax at end\n",
    "        return out\n",
    "    \n",
    "model = NeuralNet2(input_size=28*28, hidden_size=5, num_classes=3)\n",
    "criterion = nn.CrossEntropyLoss() #applies softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in case of binary classification ( is it a dog or not)\n",
    "class NeuralNet1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet1, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        # sigmoid at end\n",
    "        y_pred = torch.sigmoid(out)\n",
    "        return y_pred\n",
    "    \n",
    "model = NeuralNet1(input_size=28*28, hidden_size=5)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "1. Step function\n",
    "2. Sigmoid\n",
    "3. TanH\n",
    "4. ReLU\n",
    "5. Leaky ReLU\n",
    "6. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1 (create nn modeules)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2 (use activation functiosn directly in forward pass)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__ (self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
